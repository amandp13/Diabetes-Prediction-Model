# -*- coding: utf-8 -*-
"""Diabetes_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12xIVvV7j4E5d2lk7ONo9-rvng0ucWK0w

### **Installing Dependencies.**
"""

! pip install pyspark

"""### **Running Spark Session**"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("spark").getOrCreate()

"""## **Clone Diabetes Dataset**"""

! git clone https://github.com/education454/diabetes_dataset

! ls diabetes_dataset

df = spark.read.csv('/content/diabetes_dataset/diabetes.csv', header=True, inferSchema=True)

df.show()

df.printSchema()

print((df.count(), len(df.columns)))

df.groupby('Outcome').count().show()

df.describe().show()

"""## **Data Cleaning**|"""

# Checking all the Null values in the Dataset.
for col in df.columns:
  print(col+ ":", df[df[col].isNull()].count())

# Finding total number of 0s entry in colums.
def count_zero():
  column_list= ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
  for i in column_list:
    print(i+ ":", df[df[i] == 0].count())

count_zero()

# Upadating Null with mean value.
from pyspark.sql.functions import *
for i in df.columns[1:6]:
  data = df.agg({i:'mean'}).first()[0]
  print("Mean value for {} is {}".format(i,int(data)))
  df = df.withColumn(i, when(df[i]== 0, int(data)).otherwise(df[i]))

df.show()

# Finding Co-Relation among Input and Output Values, Finding best feature for our model.
for col in df.columns:
  print("Corelation to outcome for {} is {}".format(col, df.stat.corr('Outcome',col)))

"""## **Feature Selection**"""

# we will Choose all input features for our model as similar co_relation 
from pyspark.ml.feature import VectorAssembler
# defining a new feature column.
assembler = VectorAssembler(inputCols=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age'], outputCol='features')
output_data = assembler.transform(df) # produce a new column 'features

output_data.printSchema()

output_data.show()

"""## **Build & Train Model**"""

# We will now use Logistic Regression Algorithm model.
from pyspark.ml.classification import LogisticRegression
# selecting featues and outcome column.
final_data = output_data.select('features', 'Outcome')

final_data.printSchema()

# Splitting the dataset into training and test part.
train, test = final_data.randomSplit([0.7,0.3])
# train our model.
models = LogisticRegression(labelCol='Outcome')
model = models.fit(train)

summary = model.summary

summary.predictions.describe().show()

"""### **Evaluate and Test Our Model**"""

# By deafault it uses Area under the ROC as performance matrix by default.
from pyspark.ml.evaluation import BinaryClassificationEvaluator
predictions = model.evaluate(test)

# By default it adds the three column.
predictions.predictions.show(10)

# using Binary classification evaluator
evaluator =BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='Outcome')
# How accurate our Model is?
evaluator.evaluate(model.transform(test))

"""## ***This Show how much accuracy our Model has.***
---
"""

# Saving our model.
model.save("Model")

# for Further use in working environment.
from pyspark.ml.classification import LogisticRegressionModel
model = LogisticRegressionModel.load('Model')

"""## **End.**"""

